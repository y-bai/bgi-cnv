#!/usr/bin/env python
# -*- coding:utf-8 -*-

"""
    File Name: online_feat_seg_opt_main.py
    Description:
    #####
    learn multiple process with lock
    
Created by Yong Bai on 2019/9/27 9:17 AM.
"""

import os
import argparse
import logging
import sys
import numpy as np
import time
import multiprocessing
import pysam
import h5py

sys.path.append("..")
# from online_feat_gen.large_file_reader import CachedLineList
from cnv_utils import load_gap
from cnv_utils import find_seg, seq_slide

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# global variable
whl_f_mat_shared = None
whl_base_pos_shared = None
whl_f_mat_shape0 = None
whl_f_mat_shape1 = None
min_r = 0
win_len = 0


def mp_init(l):
    global lock
    lock = l


def mt_get_feat_wrapper(in_args):
    return get_feat_seg(*in_args)


def get_feat_seg(seg_s, seg_end):
    lock.acquire()
    seg_len = seg_end - seg_s
    # logger.info('processing at {}, len {}'.format(seg_s, seg_len))
    shared_mat = np.frombuffer(whl_f_mat_shared.get_obj(),
                               dtype=np.float32).reshape((whl_f_mat_shape0.value, whl_f_mat_shape1.value))
    shared_base_pos_arr = np.frombuffer(whl_base_pos_shared.get_obj(), dtype=np.int32)

    i_feat_mat = shared_mat[seg_s:seg_end]
    i_base_cov = shared_base_pos_arr[seg_s:seg_end]
    valide_len = len(i_base_cov[i_base_cov > 0])

    if valide_len == 0:
        # no reads cover
        lock.release()
        return seg_s, seg_end, seg_len, 1, None
    elif valide_len < win_len.value * min_r.value:
        # the number of reads coverage is not enough
        lock.release()
        return seg_s, seg_end, seg_len, 2, None
    else:
        if int(seg_len) < win_len.value:
            # ensure the win_size length
            i_feat_mat = shared_mat[(seg_end - win_len.value):seg_end]
        # ok, requirements satisfied
        # normalize
        # i_feat_mat[:, 0] = np.log(1+i_feat_mat[:, 0])
        # i_feat_mat[:, 0:3] = np.log(1 + i_feat_mat[:, 0:3])
        i_x_max = np.max(i_feat_mat, axis=0)
        i_x_max[i_x_max == 0] = 1
        i_feat_mat = i_feat_mat * 1.0 / i_x_max.reshape(1, len(i_x_max))
        lock.release()
        return seg_s, seg_end, seg_len, 3, i_feat_mat


def main(args):
    sample_id = args.sample_id
    chr_id = args.chr_id

    min_ratio = args.min_ratio
    win_size = args.win_size
    step_size = args.step_size
    n_proc = args.n_proc
    n_features = args.n_features

    ref_fasta_fn = args.ref_fa_f

    online_out_root_dir = args.online_out_root_dir

    n_seg_range = args.n_seg_range

    online_out_sample_subdir = os.path.join(online_out_root_dir, sample_id)
    if not os.path.isdir(online_out_sample_subdir):
        os.mkdir(online_out_sample_subdir)
    online_out_sample_data_dir = os.path.join(online_out_sample_subdir, 'data')
    if not os.path.isdir(online_out_sample_data_dir):
        os.mkdir(online_out_sample_data_dir)

    # this was generated by STEP 20
    in_online_sample_data_fn = os.path.join(online_out_sample_data_dir,
                                            '{}_chr{}_features_whole.h5'.format(sample_id, chr_id))
    if not os.path.exists(in_online_sample_data_fn):
        raise FileNotFoundError('input feature file does not exist. {}'.format(in_online_sample_data_fn))

    # load reference fasta file to get the chr len
    if not os.path.exists(ref_fasta_fn):
        raise FileNotFoundError('Reference fasta file does not exist. {}'.format(ref_fasta_fn))
    logger.info('loading Reference fasta file...')
    ref_fa_obj = pysam.FastaFile(ref_fasta_fn)
    # reference base
    logger.info('Loading reference sequence for sample {} chr: {}...'.format(sample_id, sample_id))
    rb_base_chr = ref_fa_obj.fetch('chr' + chr_id)
    chr_len = len(rb_base_chr)

    # load gap information
    # index starting at 0
    ref_gap_obj = load_gap()
    ref_gap_chr = ref_gap_obj.loc[ref_gap_obj['CHROM'] == 'chr' + chr_id,
                                  ['START', 'END']] if not ref_gap_obj.empty else None

    fil_pos = np.array([], dtype=np.int)
    if ref_gap_chr is not None:
        for _, i_row in ref_gap_chr.iterrows():
            # END is excluded
            fil_pos = np.concatenate((fil_pos, np.arange(i_row['START'], i_row['END'])))
    rb_base_pos = np.ones(chr_len, dtype=int)
    rb_base_pos[fil_pos] = 0

    out_feat_segs_fn = os.path.join(
        online_out_sample_data_dir,
        'win{0}_step{1}_r{2:.2f}_chr{3}_seg'.format(win_size, step_size, min_ratio, chr_id))
    if n_seg_range != 'a':  # if not whole chr
        seg_range = np.array(n_seg_range.strip().split('-'), dtype=np.int)
        seg_start_, seg_end_ = seg_range[0], seg_range[1]
        rb_base_pos = rb_base_pos[seg_start_:seg_end_]
        out_feat_segs_fn = out_feat_segs_fn + '_p-' + n_seg_range
        seg_values, seg_starts, seg_lengths = find_seg(rb_base_pos)
        seg_starts = seg_starts + seg_start_
    else:
        seg_values, seg_starts, seg_lengths = find_seg(rb_base_pos)
        out_feat_segs_fn = out_feat_segs_fn + '_a'

    assert len(seg_values) == len(seg_starts) == len(seg_lengths)

    seg_gap_inds = np.where(seg_values == 0)[0]
    seg_val_len_less_inds = np.where((seg_values == 1) & (seg_lengths < win_size))[0]
    seg_val_normal_inds = np.where((seg_values == 1) & (seg_lengths >= win_size))[0]

    logger.info('calculating gap segments for sample {}, chr {}...'.format(sample_id, chr_id))
    gap_out = np.array([[i_gap_start, i_gap_start + i_gap_lens, i_gap_lens, 0]
                        for i_gap_start, i_gap_lens in
                        zip(seg_starts[seg_gap_inds], seg_lengths[seg_gap_inds])], dtype=int)

    if len(gap_out) > 0:
        logger.info('saving gap info into h5 file...')
        with h5py.File(out_feat_segs_fn + '_gap.h5', 'w') as h5_out:
            h5_out.create_dataset('gap', data=gap_out, maxshape=(None, 4), compression="gzip", compression_opts=9,
                                  dtype=np.int32)  # compression="gzip", compression_opts=9

    logger.info('calculating {} bp-long {} bp step feature segments sample {}, chr {}...'.format(
        win_size, step_size, sample_id, chr_id))
    # seg len < win_size
    val_seg_len_less_starts = seg_starts[seg_val_len_less_inds]
    val_seg_len_less_lens = seg_lengths[seg_val_len_less_inds]
    val_seg_len_less_end = val_seg_len_less_starts + val_seg_len_less_lens
    val_seg_poss_zip = list(zip(val_seg_len_less_starts, val_seg_len_less_end))

    # slice the seg into win_size
    val_seg_normal_starts = seg_starts[seg_val_normal_inds]
    val_seg_normal_lens = seg_lengths[seg_val_normal_inds]

    val_normal_slice_starts = [seq_slide(i_seg_len, win_size, step_size)
                               for i_seg_len in val_seg_normal_lens]

    val_nomarl_slices = [(val_seg_normal_starts[i] + i_slice_start,
                          val_seg_normal_starts[i] + i_slice_start + win_size,
                          val_seg_normal_starts[i] + end_start, remain_len)
                         for i, (i_slice_start, end_start, remain_len) in enumerate(val_normal_slice_starts)]

    for i_seg_norm_starts, i_seg_norm_ends, end_start, remain_len in val_nomarl_slices:
        val_seg_poss_zip.extend(list(zip(i_seg_norm_starts, i_seg_norm_ends)))
        if remain_len > 0:
            val_seg_poss_zip.append((end_start, end_start + remain_len))

    # first load the whole features into the memory as shared variable
    logger.info('loading whole feature for sample {}, chr {}...'.format(sample_id, chr_id))
    with h5py.File(in_online_sample_data_fn, 'r') as h5_in:
        whl_f_mat = h5_in.get('feature').value
        base_pos_idx = h5_in.get('base_cov_pos_idx').value
    logger.info('feature shape {}, base cov shape {}'.format(whl_f_mat.shape, base_pos_idx.shape))

    logger.info('creating shared data...')
    global whl_f_mat_shape0
    whl_f_mat_shape0 = multiprocessing.Value('i', whl_f_mat.shape[0])

    global whl_f_mat_shape1
    whl_f_mat_shape1 = multiprocessing.Value('i', whl_f_mat.shape[1])

    global whl_f_mat_shared
    whl_f_mat_shared = multiprocessing.Array('f', whl_f_mat.shape[0] * whl_f_mat.shape[1])
    whl_f_mat_arr_np = np.frombuffer(whl_f_mat_shared.get_obj(), dtype=np.float32)
    whl_f_mat_arr = whl_f_mat_arr_np.reshape((whl_f_mat.shape[0], whl_f_mat.shape[1]))
    np.copyto(whl_f_mat_arr, whl_f_mat)

    global whl_base_pos_shared
    whl_base_pos_shared = multiprocessing.Array('i', base_pos_idx.shape[0])
    whl_base_pos_arr_np = np.frombuffer(whl_base_pos_shared.get_obj(), dtype=np.int32)
    np.copyto(whl_base_pos_arr_np, base_pos_idx)

    global win_len
    win_len = multiprocessing.Value('i', win_size)

    global min_r
    min_r = multiprocessing.Value('f', min_ratio)

    del whl_f_mat
    del base_pos_idx

    # the total number of segments
    n_seg_lens_ = len(val_seg_poss_zip)
    logger.info('The total number of segments is {} '.format(n_seg_lens_))
    n_chk_size = 500

    logger.info('starting segmenting...')
    locker = multiprocessing.Lock()
    with multiprocessing.Pool(n_proc, initializer=mp_init, initargs=(locker,)) as p:
        results = p.imap(mt_get_feat_wrapper, val_seg_poss_zip, chunksize=n_chk_size)  # 16 core ~ 80G memory
        time.sleep(10)

        unpredictable_meta_arr = []
        predictable_meta_arr = []
        predictable_feat_arr = []

        pred_save_idx = -1
        unpred_save_idx = -1

        saved_len = 1024 * 32 * 4

        pred_start_p = np.inf
        pred_end_p = -np.inf

        unpred_start_p = np.inf
        unpred_end_p = -np.inf

        for i_idx, (i_seg_s, i_seg_end, i_seg_len, i_indicator, i_feat_mat) in enumerate(results):
            if i_idx % n_chk_size == 0:
                logger.info('finished at {}, {} of {}, type {}'.format(i_seg_s, i_idx, n_seg_lens_, i_indicator))

            if i_indicator == 3:
                pred_start_p = int(np.minimum(i_seg_s, pred_start_p))
                pred_end_p = int(np.maximum(i_seg_end, pred_end_p))

                predictable_meta_arr.append(np.array([i_seg_s, i_seg_end, i_seg_len, i_indicator], dtype=np.int32))
                predictable_feat_arr.append(i_feat_mat)

                if len(predictable_meta_arr) >= saved_len:
                    pred_save_idx += 1
                    logger.info('writing {} pred results, file index: {}'.format(
                        len(predictable_meta_arr), pred_save_idx))

                    # tn: total number of the segments, sn: the number of segments in the current file
                    i_pred_fn = out_feat_segs_fn + '_pred_idx{}_tn{}_sn{}_s{}_e{}.h5'.format(
                        pred_save_idx, n_seg_lens_, len(predictable_meta_arr), pred_start_p, pred_end_p)
                    if os.path.exists(i_pred_fn):
                        os.remove(i_pred_fn)
                    with h5py.File(i_pred_fn, 'w') as pred_h5:
                        pred_h5.create_dataset('pred_meta', data=predictable_meta_arr,
                                               compression="gzip", compression_opts=4)
                        pred_h5.create_dataset('pred_feat', data=predictable_feat_arr,
                                               compression="gzip", compression_opts=4)
                    predictable_meta_arr.clear()
                    predictable_feat_arr.clear()

                    pred_start_p = np.inf
                    pred_end_p = -np.inf

            else:

                unpred_start_p = int(np.minimum(i_seg_s, unpred_start_p))
                unpred_end_p = int(np.maximum(i_seg_end, unpred_end_p))

                unpredictable_meta_arr.append(np.array([i_seg_s, i_seg_end, i_seg_len, i_indicator], dtype=np.int32))

                if len(unpredictable_meta_arr) >= saved_len:
                    unpred_save_idx += 1
                    logger.info('writing {} unpred results, file index: {}'.format(
                        len(unpredictable_meta_arr), unpred_save_idx))
                    i_unpred_fn = out_feat_segs_fn + '_unpred_idx{}_tn{}_sn{}_s{}_e{}.h5'.format(
                        unpred_save_idx, n_seg_lens_, len(unpredictable_meta_arr), unpred_start_p, unpred_end_p)
                    if os.path.exists(i_unpred_fn):
                        os.remove(i_unpred_fn)
                    with h5py.File(i_unpred_fn, 'w') as unpred_h5:
                        unpred_h5.create_dataset('unpred_meta', data=unpredictable_meta_arr,
                                                 compression="gzip", compression_opts=4)
                    unpredictable_meta_arr.clear()

                    unpred_start_p = np.inf
                    unpred_end_p = -np.inf

        if len(predictable_meta_arr) > 0:
            logger.info('writing {} pred results, file index: {}'.format(
                len(predictable_meta_arr), pred_save_idx + 1))

            i_pred_fn = out_feat_segs_fn + '_pred_idx{}_tn{}_sn{}_s{}_e{}.h5'.format(
                pred_save_idx + 1, n_seg_lens_, len(predictable_meta_arr), pred_start_p, pred_end_p)
            if os.path.exists(i_pred_fn):
                os.remove(i_pred_fn)
            with h5py.File(i_pred_fn, 'w') as pred_h5:
                pred_h5.create_dataset('pred_meta', data=predictable_meta_arr,
                                       compression="gzip", compression_opts=4)
                pred_h5.create_dataset('pred_feat', data=predictable_feat_arr,
                                       compression="gzip", compression_opts=4)
        if len(unpredictable_meta_arr) > 0:
            logger.info('writing {} unpred results, file index: {}'.format(
                len(unpredictable_meta_arr), unpred_save_idx + 1))
            i_unpred_fn = out_feat_segs_fn + '_unpred_idx{}_tn{}_sn{}_s{}_e{}.h5'.format(
                unpred_save_idx + 1, n_seg_lens_, len(unpredictable_meta_arr), unpred_start_p, unpred_end_p)
            if os.path.exists(i_unpred_fn):
                os.remove(i_unpred_fn)
            with h5py.File(i_unpred_fn, 'w') as unpred_h5:
                unpred_h5.create_dataset('unpred_meta', data=unpredictable_meta_arr,
                                         compression="gzip", compression_opts=4)

    logger.info('Done, whole feature-segments for sample {} chr {} saving at {}'.format(
        sample_id, chr_id, out_feat_segs_fn))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Generate online feature segmentation for whole chromosome')

    parser.add_argument(
        "-s",
        "--sample_id",
        type=str,
        help="input sample id",
        required=True)

    parser.add_argument(
        "-c",
        "--chr_id",
        type=str,
        default='A',
        help="chromosome id, provide like '1'")

    parser.add_argument(
        "-w",
        "--win_size",
        type=int,
        default=1000,
        help='window size. This will be hyperparamter')

    parser.add_argument(
        "-r",
        '--min_ratio',
        type=float,
        default=0.1,
        help="cnv region that has read coverage less than the ratio will be filtered out. This will be hyperparameter")

    parser.add_argument(
        "-p",
        "--step_size",
        type=int,
        default=200,
        help="step size when sliding window")

    parser.add_argument(
        "-u",
        "--n_features",
        type=int,
        default=13,
        help="the number of features")

    parser.add_argument(
        "-x",
        "--n_seg_range",
        type=str,
        default='a',
        help="segment range, 'a' for whole chr, otherwise seg_s-seg_e, eg, 10000-30000")

    parser.add_argument(
        "-t",
        "--n_proc",
        type=int,
        default=20,
        help="the number of process")

    parser.add_argument(
        "-o",
        "--online_out_root_dir",
        type=str,
        default='/zfssz2/ST_MCHRI/BIGDATA/PROJECT/NIPT_CNV/f_cnv_out/online',
        help="online output directory")

    parser.add_argument("-f", '--ref_fa_f', type=str, help='reference fasta file')
    args = parser.parse_args()
    logger.info('input parameters: {}'.format(args))
    main(args)
