{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Description:\n",
    "\n",
    "This is used to analyze the results of training model by cv.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_root_dir = '/zfssz2/ST_MCHRI/BIGDATA/PROJECT/NIPT_CNV/1dcnn_resnet'\n",
    "cv_res_dir = os.path.join(res_root_dir, 'models2')\n",
    "\n",
    "# this is the results from keras cvslogger callback\n",
    "cv_metrics_dir = os.path.join(cv_res_dir,'model_metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_score(fname):\n",
    "    df_score = pd.read_csv(fname)\n",
    "    res = df_score.loc[df_score['val_acc'].idxmax()]\n",
    "    return int(res['epoch']+1), res['acc'], res['loss'], res['val_acc'], res['val_loss']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataframe to save best result\n",
    "col_name = ['batch_size', 'epoch', 'learn_rate', \n",
    "            'learn_rate_decay', 'filters', 'drop', \n",
    "            'fc_size', 'blocks', 'kernel_regular',\n",
    "            'best_epoch', 'train_loss', 'train_acc', 'val_loss', 'val_acc']\n",
    "\n",
    "# we also need to determine \n",
    "#     1, the use of fully connection layers \n",
    "#        or global_average_pooling layers.\n",
    "#     2, the use of attention layers or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = []\n",
    "epoch = []\n",
    "lr = []\n",
    "lr_decay = []\n",
    "filters = []\n",
    "drop = []\n",
    "fc_size = []\n",
    "blocks = []\n",
    "kernel_reguler = []\n",
    "best_epoch = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "re_exp = r'[-+]?\\d*\\.\\d+|\\d+|None'\n",
    "for fname in glob.glob(cv_metrics_dir+'/*'):\n",
    "    sc = get_high_score(fname)\n",
    "    hps_str = os.path.basename(fname).split('-')[0]\n",
    "    hps_lst = re.findall(re_exp, hps_str)\n",
    "    batch_size.append(int(hps_lst[0]))\n",
    "    epoch.append(int(hps_lst[1]))\n",
    "    lr.append(float(hps_lst[2]))\n",
    "    lr_decay.append(float(hps_lst[3]))\n",
    "    filters.append(int(hps_lst[4]))\n",
    "    drop.append(float(hps_lst[5]))\n",
    "    fc_size.append(int(hps_lst[6]))\n",
    "    blocks.append(int(hps_lst[7]))\n",
    "    if len(hps_lst) <= 8 or hps_lst[8] == 'None':\n",
    "        kernel_reguler.append(np.NAN)\n",
    "    elif len(hps_lst) > 8:\n",
    "        kernel_reguler.append(float(hps_lst[8]))\n",
    "    best_epoch.append(sc[0])\n",
    "    train_loss.append(sc[1])\n",
    "    train_acc.append(sc[2])\n",
    "    val_loss.append(sc[3])\n",
    "    val_acc.append(sc[4])\n",
    "df_res = pd.DataFrame(data={'batch_size':batch_size, 'epoch':epoch, 'learn_rate':lr, \n",
    "            'learn_rate_decay': lr_decay, 'filters':filters, 'drop':drop, \n",
    "            'fc_size':fc_size, 'blocks':blocks, 'kernel_regular':kernel_reguler,\n",
    "            'best_epoch':best_epoch, 'train_loss':train_loss, \n",
    "            'train_acc':train_acc, 'val_loss':val_loss, 'val_acc':val_acc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epoch</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>learn_rate_decay</th>\n",
       "      <th>filters</th>\n",
       "      <th>drop</th>\n",
       "      <th>fc_size</th>\n",
       "      <th>blocks</th>\n",
       "      <th>kernel_regular</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.4</td>\n",
       "      <td>256</td>\n",
       "      <td>443</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>0.995155</td>\n",
       "      <td>0.011271</td>\n",
       "      <td>0.970613</td>\n",
       "      <td>0.218984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.8</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>256</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333209</td>\n",
       "      <td>10.740725</td>\n",
       "      <td>0.333756</td>\n",
       "      <td>10.738578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.4</td>\n",
       "      <td>64</td>\n",
       "      <td>4441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>0.797039</td>\n",
       "      <td>0.498250</td>\n",
       "      <td>0.817549</td>\n",
       "      <td>0.453982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>0.981213</td>\n",
       "      <td>0.053976</td>\n",
       "      <td>0.966327</td>\n",
       "      <td>0.150773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>512</td>\n",
       "      <td>50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>128</td>\n",
       "      <td>0.4</td>\n",
       "      <td>32</td>\n",
       "      <td>441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333248</td>\n",
       "      <td>10.734761</td>\n",
       "      <td>0.333933</td>\n",
       "      <td>10.735728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>512</td>\n",
       "      <td>40</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>4441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333170</td>\n",
       "      <td>10.735352</td>\n",
       "      <td>0.333933</td>\n",
       "      <td>10.735728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333160</td>\n",
       "      <td>10.742233</td>\n",
       "      <td>0.333933</td>\n",
       "      <td>10.735728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1024</td>\n",
       "      <td>50</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>128</td>\n",
       "      <td>0.4</td>\n",
       "      <td>256</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333227</td>\n",
       "      <td>10.721587</td>\n",
       "      <td>0.333727</td>\n",
       "      <td>10.739059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>256</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333505</td>\n",
       "      <td>10.716657</td>\n",
       "      <td>0.332371</td>\n",
       "      <td>10.760908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>256</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>0.993998</td>\n",
       "      <td>0.013961</td>\n",
       "      <td>0.969874</td>\n",
       "      <td>0.193111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.4</td>\n",
       "      <td>256</td>\n",
       "      <td>4443</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>0.990198</td>\n",
       "      <td>0.022899</td>\n",
       "      <td>0.970966</td>\n",
       "      <td>0.161303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  epoch  learn_rate  learn_rate_decay  filters  drop  fc_size  \\\n",
       "0          256     50       0.001               0.8       16   0.4      256   \n",
       "1          256     20       0.010               0.8      128   0.5      256   \n",
       "2          128     20       0.010               0.5       16   0.4       64   \n",
       "3          256     40       0.001               0.8       16   0.2       64   \n",
       "4          512     50       0.010               0.1      128   0.4       32   \n",
       "5          512     40       0.100               0.8      128   0.5       64   \n",
       "6          256     20       0.100               0.8       16   0.2       64   \n",
       "7         1024     50       0.100               0.8      128   0.4      256   \n",
       "8         1024     20       0.010               0.5      128   0.2      256   \n",
       "9          256     40       0.001               0.1       16   0.5      256   \n",
       "10         256     20       0.001               0.5      128   0.4      256   \n",
       "\n",
       "    blocks  kernel_regular  best_epoch  train_loss  train_acc  val_loss  \\\n",
       "0      443             NaN          49    0.995155   0.011271  0.970613   \n",
       "1       43             NaN           1    0.333209  10.740725  0.333756   \n",
       "2     4441             NaN          20    0.797039   0.498250  0.817549   \n",
       "3       43             NaN          40    0.981213   0.053976  0.966327   \n",
       "4      441             NaN           1    0.333248  10.734761  0.333933   \n",
       "5     4441             NaN           1    0.333170  10.735352  0.333933   \n",
       "6       41             NaN           1    0.333160  10.742233  0.333933   \n",
       "7       41             NaN           1    0.333227  10.721587  0.333727   \n",
       "8       43             NaN           1    0.333505  10.716657  0.332371   \n",
       "9       43             NaN          27    0.993998   0.013961  0.969874   \n",
       "10    4443             NaN          20    0.990198   0.022899  0.970966   \n",
       "\n",
       "      val_acc  \n",
       "0    0.218984  \n",
       "1   10.738578  \n",
       "2    0.453982  \n",
       "3    0.150773  \n",
       "4   10.735728  \n",
       "5   10.735728  \n",
       "6   10.735728  \n",
       "7   10.739059  \n",
       "8   10.760908  \n",
       "9    0.193111  \n",
       "10   0.161303  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
