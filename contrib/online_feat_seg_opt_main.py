#!/usr/bin/env python
# -*- coding:utf-8 -*-

"""
    File Name: online_feat_seg_opt_main.py
    Description:
    #####
    learn multiple process with lock
    
Created by Yong Bai on 2019/9/27 9:17 AM.
"""

import os
import argparse
import logging
import sys
import numpy as np
import time
import ctypes
from functools import partial
import multiprocessing
from multiprocessing.pool import ThreadPool
import pysam
import h5py

sys.path.append("..")
# from online_feat_gen.large_file_reader import CachedLineList
from cnv_utils import load_gap
from cnv_utils import find_seg, seq_slide

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# global variable
min_ratio = 0


def init(l, h5_in_handle):
    global lock
    lock = l
    global h5_in
    h5_in = h5_in_handle


def multi_read_h5_chunk_wrapper(in_params):
    return read_h5_chunk(*in_params)


def read_h5_chunk(chunk_start, chunk_len):
    # x = np.empty((0, 13), dtype=np.float32)
    # with h5py.File(in_online_sample_data_fn, 'r') as h5_in:
    lock.acquire()
    d_name = 'chk_{}_{}'.format(chunk_start, chunk_len)
    logger.info('read chunk: {}'.format(d_name))
    x = h5_in.get(d_name).value
    lock.release()
    return chunk_start, x


def main(args):

    sample_id = args.sample_id
    chr_id = args.chr_id

    global min_ratio
    min_ratio = args.min_ratio
    win_size = args.win_size
    step_size = args.step_size
    n_proc = args.n_proc
    n_features = args.n_features

    ref_fasta_fn = args.ref_fa_f

    online_out_root_dir = args.online_out_root_dir

    online_out_sample_subdir = os.path.join(online_out_root_dir, sample_id)
    if not os.path.isdir(online_out_sample_subdir):
        os.mkdir(online_out_sample_subdir)
    online_out_sample_data_dir = os.path.join(online_out_sample_subdir, 'data')
    if not os.path.isdir(online_out_sample_data_dir):
        os.mkdir(online_out_sample_data_dir)

    # this was generated by STEP 20
    in_online_sample_data_fn = os.path.join(online_out_sample_data_dir,
                                            '{}_chr{}_features_whole.h5'.format(sample_id, chr_id))
    if not os.path.exists(in_online_sample_data_fn):
        raise FileNotFoundError('input feature file does not exist. {}'.format(in_online_sample_data_fn))

    out_feat_segs_fn = os.path.join(
        online_out_sample_data_dir,
        'win{0}_step{1}_r{2:.2f}_chr{3}_features_seg.h5'.format(win_size, step_size, min_ratio, chr_id))
    if os.path.exists(out_feat_segs_fn):
        os.remove(out_feat_segs_fn)

    # load reference fasta file to get the chr len
    if not os.path.exists(ref_fasta_fn):
        raise FileNotFoundError('Reference fasta file does not exist. {}'.format(ref_fasta_fn))
    logger.info('loading Reference fasta file...')
    ref_fa_obj = pysam.FastaFile(ref_fasta_fn)
    # reference base
    logger.info('Loading reference sequence for sample {} chr: {}...'.format(sample_id, sample_id))
    rb_base_chr = ref_fa_obj.fetch('chr' + chr_id)
    chr_len = len(rb_base_chr)

    # load gap information
    ref_gap_obj = load_gap()
    ref_gap_chr = ref_gap_obj.loc[ref_gap_obj['CHROM'] == 'chr' + chr_id,
                                  ['START', 'END']] if not ref_gap_obj.empty else None

    fil_pos = np.array([], dtype=np.int)
    if ref_gap_chr is not None:
        for _, i_row in ref_gap_chr.iterrows():
            # END is excluded
            fil_pos = np.concatenate((fil_pos, np.arange(i_row['START'], i_row['END'])))
    rb_base_pos = np.ones(chr_len, dtype=int)
    rb_base_pos[fil_pos] = 0

    seg_values, seg_starts, seg_lengths = find_seg(rb_base_pos)
    assert len(seg_values) == len(seg_starts) == len(seg_lengths)

    seg_gap_inds = np.where(seg_values == 0)[0]
    seg_val_len_less_inds = np.where((seg_values == 1) & (seg_lengths < win_size))[0]
    seg_val_normal_inds = np.where((seg_values == 1) & (seg_lengths >= win_size))[0]

    logger.info('calculating gap segments for sample {}, chr {}...'.format(sample_id, chr_id))
    gap_out = np.array([[i_gap_start, i_gap_start + i_gap_lens, i_gap_lens, 0]
                        for i_gap_start, i_gap_lens in
                        zip(seg_starts[seg_gap_inds], seg_lengths[seg_gap_inds])], dtype=int)

    logger.info('saving gap info into h5 file...')
    with h5py.File(out_feat_segs_fn, 'w') as h5_out:
        h5_out.create_dataset('unpredictable_meta', data=gap_out,
                              compression="gzip", compression_opts=9)

    logger.info('calculating {} bp-long {} bp step feature maps sample {}, chr {}...'.format(
        win_size, step_size, sample_id, chr_id))
    # seg len < win_size
    val_seg_len_less_starts = seg_starts[seg_val_len_less_inds]
    val_seg_len_less_lens = seg_lengths[seg_val_len_less_inds]
    val_seg_len_less_end = val_seg_len_less_starts + val_seg_len_less_lens
    val_seg_poss_zip = list(zip(val_seg_len_less_starts, val_seg_len_less_end))

    # slice the seg into win_size
    val_seg_normal_starts = seg_starts[seg_val_normal_inds]
    val_seg_normal_lens = seg_lengths[seg_val_normal_inds]

    val_normal_slice_starts = [seq_slide(i_seg_len, win_size, step_size)
                               for i_seg_len in val_seg_normal_lens]

    val_nomarl_slices = [(val_seg_normal_starts[i] + i_slice_start,
                          val_seg_normal_starts[i] + i_slice_start + win_size,
                          end_start, remain_len)
                         for i, (i_slice_start, end_start, remain_len) in enumerate(val_normal_slice_starts)]

    for i_seg_norm_starts, i_seg_norm_ends, end_start, remain_len in val_nomarl_slices:
        val_seg_poss_zip.extend(list(zip(i_seg_norm_starts, i_seg_norm_ends)))
        if remain_len > 0:
            val_seg_poss_zip.append((end_start, end_start + remain_len))

    # first load the whole features into the memory as shared variable
    logger.info('loading whole feature for sample {}, chr {}...'.format(sample_id, chr_id))
    chunk_len = int(1e5)
    start_p, last_start_p, last_len = seq_slide(chr_len, chunk_len, chunk_len)
    chunk_start_lens = list(zip(start_p, np.repeat(chunk_len, len(start_p))))
    if last_len > 0:
        chunk_start_lens.append((last_start_p, last_len))

    h5_in_handle = h5py.File(in_online_sample_data_fn, 'r')
    whl_f_amt = np.empty((0, n_features), dtype=np.float32)
    locker = multiprocessing.Lock()
    with multiprocessing.Pool(n_proc, initializer=init, initargs=(locker, h5_in_handle)) as p:
        # m = multiprocessing.Manager()
        # l = m.Lock()
        # func = partial(multi_read_h5_chunk_wrapper, l)
        results = p.imap(multi_read_h5_chunk_wrapper, chunk_start_lens)  #
        time.sleep(10)
        # whl_f_amt = np.stack(results, axis=0)
        for i_chk_s, i_fmat in results:
            logger.info('finished at: {}'.format(i_chk_s))
            whl_f_amt = np.concatenate((whl_f_amt, i_fmat), axis=0)

    logger.info('shape:{}'.format(whl_f_amt.shape))
    h5_in_handle.close()

    # with h5py.File(in_online_sample_data_fn, 'r') as h5_in:
    #     for idx, (i_chunk_start, i_chunk_len) in enumerate(chunk_start_lens):  # to keep the order when reading data
    #         dataset_name = 'chk_{}_{}'.format(i_chunk_start, i_chunk_len)
    #         if idx == 0:
    #             whl_chr_famt = h5_in[dataset_name]
    #         else:
    #             whl_chr_famt = np.concatenate((whl_chr_famt, h5_in[dataset_name]), axis=0)

    logger.info('Done, whole features for {} chr {} saving at {}'.format(
        sample_id, chr_id, out_feat_segs_fn))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Generate online feature segmentation for whole chromosome')

    parser.add_argument(
        "-s",
        "--sample_id",
        type=str,
        help="input sample id",
        required=True)

    parser.add_argument(
        "-c",
        "--chr_id",
        type=str,
        default='A',
        help="chromosome id, provide like '1'")

    parser.add_argument(
        "-w",
        "--win_size",
        type=int,
        default=1000,
        help='window size. This will be hyperparamter')

    parser.add_argument(
        "-r",
        '--min_ratio',
        type=float,
        default=0.1,
        help="cnv region that has read coverage less than the ratio will be filtered out. This will be hyperparameter")

    parser.add_argument(
        "-p",
        "--step_size",
        type=int,
        default=200,
        help="step size when sliding window")

    parser.add_argument(
        "-u",
        "--n_features",
        type=int,
        default=13,
        help="the number of features")

    parser.add_argument(
        "-t",
        "--n_proc",
        type=int,
        default=20,
        help="the number of process")

    parser.add_argument(
        "-o",
        "--online_out_root_dir",
        type=str,
        default='/zfssz2/ST_MCHRI/BIGDATA/PROJECT/NIPT_CNV/f_cnv_out/online',
        help="online output directory")

    parser.add_argument("-f", '--ref_fa_f', type=str, help='reference fasta file')
    args = parser.parse_args()
    logger.info('input parameters: {}'.format(args))
    main(args)