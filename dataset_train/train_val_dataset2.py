#!/usr/bin/env python
# -*- coding:utf-8 -*-

"""
    File Name: train_val_dataset2.py
    Description: split dataset for train and validation.
    the dataset was  generated by model_crt_dataset4generator2.py
    
Created by Yong Bai on 2019/9/18 5:14 PM.
"""

import os
import numpy as np
import multiprocessing as mp
import argparse
import logging
import warnings

with warnings.catch_warnings():
    warnings.filterwarnings("ignore", category=FutureWarning)
    import h5py

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# global var
trains_h5_fn = None


def mp_init(l):
    global lock
    lock = l


def get_sample_from_h5(out_index):
    lock.acquire()
    sort_idx = np.argsort(out_index)
    back_idx = np.argsort(sort_idx)
    sort_v = out_index[sort_idx]
    list_ind = list(sort_v)
    with h5py.File(trains_h5_fn, 'r') as trains_h5:
        x = trains_h5['x'][list_ind][...]
        strs_y = trains_h5['str_y'][list_ind][...]
        cnv_tpyes = [x.split('|')[5] for x in strs_y]
        y = np.array([1 if cnv_tpye == 'DEL' else 2 if cnv_tpye == 'DUP' else 0 for cnv_tpye in cnv_tpyes])
    lock.release()
    return x[back_idx], y[back_idx]


def create_train_val_dataset(train_data_h5_fn, out_train_fn, out_val_fn, win_size,
                             train_ratio=0.8, nb_cls_prop='1:1:1', n_features=13, n_proc=20):
    logger.info('loading the whole train data sample list file, will take a while...')

    global trains_h5_fn
    trains_h5_fn = train_data_h5_fn
    trains_data_h5 = h5py.File(train_data_h5_fn, 'r')

    trains_x_dset = trains_data_h5['x']
    len_samples = trains_x_dset.attrs['n_sample']
    trains_y_dset = trains_data_h5['str_y']

    assert trains_x_dset.shape[0] == len_samples == trains_y_dset.shape[0]
    logger.info('The total number of samples {}'.format(len_samples))

    def f(x):
        cnv_tpye = x.split('|')[5]
        return 1 if cnv_tpye == 'DEL' else 2 if cnv_tpye == 'DUP' else 0

    trains_y_str_arr = trains_y_dset[...]
    trains_y_lal_arr = np.array(list(map(f, trains_y_str_arr)), dtype=np.int32)

    trains_data_h5.close()

    del_idx = np.where(trains_y_lal_arr == 1)[0]
    dup_idx = np.where(trains_y_lal_arr == 2)[0]
    neu_idx = np.where(trains_y_lal_arr == 0)[0]

    n_dels = len(del_idx)
    n_dups = len(dup_idx)
    n_neus = len(neu_idx)
    logger.info(' the number of original samples for DEL, DUP, NEU: {},{},{}'.format(n_dels, n_dups, n_neus))

    nb_cls_prop_arr = np.array([int(x) for x in nb_cls_prop.split(':')])
    if len(np.unique(nb_cls_prop_arr)) == 1:
        n_sample_arr = np.array([n_dels, n_dups, n_neus])
        n_min = np.min(n_sample_arr)
        n_f_dels = n_min
        n_f_dups = n_min
        n_f_neus = n_min
    else:
        assert n_dups >= n_dels * nb_cls_prop_arr[1]
        assert n_neus >= n_dels * nb_cls_prop_arr[2]

        n_f_dels = n_dels
        n_f_dups = n_dels * nb_cls_prop_arr[1]
        n_f_neus = n_dels * nb_cls_prop_arr[2]
    logger.info('the number to be sampled for DEL, DUP, NEU: {},{},{}'.format(n_f_dels, n_f_dups, n_f_neus))

    logger.info('sampling data sample list...')
    # np.random.seed(12345)
    # random sample the samples to make the balance data set
    sel_del_idx = np.random.choice(del_idx, size=n_f_dels, replace=False)
    sel_dup_idx = np.random.choice(dup_idx, size=n_f_dups, replace=False)
    sel_neu_idx = np.random.choice(neu_idx, size=n_f_neus, replace=False)

    logger.info('creating training data set and validation data set for dup, del and neu, respectively...')
    # split to get val data set and train dataset
    del_msk = np.random.rand(n_f_dels) < train_ratio
    train_del_indice = sel_del_idx[del_msk]
    val_del_indice = sel_del_idx[~del_msk]

    dup_msk = np.random.rand(n_f_dups) < train_ratio
    train_dup_indice = sel_dup_idx[dup_msk]
    val_dup_indice = sel_dup_idx[~dup_msk]

    neu_msk = np.random.rand(n_f_neus) < train_ratio
    train_neu_indice = sel_neu_idx[neu_msk]
    val_neu_indice = sel_neu_idx[~neu_msk]

    h5_chuncks = 4
    h5_str_dt = h5py.special_dtype(vlen=str)
    ################################################
    # training data set
    logger.info('combine training data set of dup, del and neu, and shuffling...')
    train_indice = np.concatenate((train_del_indice, train_dup_indice, train_neu_indice), axis=0)
    # sort so that it can be indexed by h5
    # train_indice = train_indice[np.argsort(train_indice)]
    np.random.shuffle(train_indice)

    len_train = len(train_indice)
    logger.info('the length of train: {}'.format(len_train))
    out_train_h5 = h5py.File(out_train_fn, 'w')
    train_index = out_train_h5.create_dataset('original_index',
                                              (len_train,),
                                              maxshape=(None,),
                                              dtype=np.int32,
                                              compression="gzip", compression_opts=4)
    train_x_dset = out_train_h5.create_dataset('x',
                                               (len_train, win_size, n_features),
                                               maxshape=(None, win_size, n_features),
                                               dtype=np.float32,
                                               chunks=(h5_chuncks, win_size, n_features),
                                               compression="gzip", compression_opts=4)
    train_y_dset = out_train_h5.create_dataset('y',
                                               (len_train,),
                                               maxshape=(None,),
                                               dtype=np.int32,
                                               compression="gzip", compression_opts=4)

    train_index.attrs['ori_train_val_n_del_dup_neu'] = [n_dels, n_dups, n_neus]
    train_index.attrs['f_train_val_n_del_dup_neu'] = [n_f_dels, n_f_dups, n_f_neus]
    train_index.attrs['train_val_ratio'] = train_ratio
    train_index.attrs['f_train'] = True
    train_index.attrs['f_n'] = len_train

    train_index[...] = train_indice

    def split_chunks(l, n):
        for k in range(0, len(l), n):
            yield l[k:k+n]

    split_trainind_arr = list(split_chunks(train_indice, 10))
    locker = mp.Lock()
    train_p = mp.Pool(n_proc, initializer=mp_init, initargs=(locker,))
    # train_p = mp.Pool(n_proc)
    train_results = train_p.imap(get_sample_from_h5, split_trainind_arr)

    i_train_start = 0
    for i, (re_x, re_y) in enumerate(train_results):
        logger.info('finished at {}/{}'.format(i + 1, len(split_trainind_arr)))
        len_t = len(re_y)
        train_x_dset[i_train_start:i_train_start+len_t] = re_x
        train_y_dset[i_train_start:i_train_start+len_t] = re_y
        i_train_start = i_train_start + len_t
    out_train_h5.close()
    train_p.close()
    train_p.join()

    ################################################
    # validation data set
    logger.info('combine validation data set of dup, del and neu, and shuffling...')
    val_indice = np.concatenate((val_del_indice, val_dup_indice, val_neu_indice), axis=0)
    # sort so that it can be indexed by h5
    # val_indice = val_indice[np.argsort(val_indice)]
    np.random.shuffle(val_indice)
    len_val = len(val_indice)
    logger.info('the length of val: {}'.format(len_val))
    out_val_h5 = h5py.File(out_val_fn, 'w')

    val_index = out_val_h5.create_dataset('original_index',
                                          (len_val,),
                                          maxshape=(None,),
                                          dtype=np.int32,
                                          compression="gzip", compression_opts=4)
    val_x_dset = out_val_h5.create_dataset('x',
                                           (len_val, win_size, n_features),
                                           maxshape=(None, win_size, n_features),
                                           dtype=np.float32,
                                           chunks=(h5_chuncks, win_size, n_features),
                                           compression="gzip", compression_opts=4)
    val_y_dset = out_val_h5.create_dataset('y',
                                           (len_val,),
                                           maxshape=(None,),
                                           dtype=np.int32,
                                           compression="gzip", compression_opts=4)

    val_index.attrs['ori_train_val_n_del_dup_neu'] = [n_dels, n_dups, n_neus]
    val_index.attrs['f_train_val_n_del_dup_neu'] = [n_f_dels, n_f_dups, n_f_neus]
    val_index.attrs['train_val_ratio'] = train_ratio
    val_index.attrs['f_train'] = False
    val_index.attrs['f_n'] = len_val
    val_index[...] = val_indice

    split_valind_arr = list(split_chunks(val_indice, 10))
    val_p = mp.Pool(n_proc, initializer=mp_init, initargs=(locker,))
    # val_p = mp.Pool(n_proc)
    val_results = val_p.imap(get_sample_from_h5, split_valind_arr)
    i_val_start = 0
    for i, (re_x, re_y) in enumerate(val_results):
        logger.info('finished at {}/{}'.format(i + 1, len(split_valind_arr)))
        len_v = len(re_y)
        val_x_dset[i_val_start: i_val_start+len_v] = re_x
        val_y_dset[i_val_start: i_val_start+len_v] = re_y
        i_val_start = i_val_start + len_v
    out_val_h5.close()
    val_p.close()
    val_p.join()

    logger.info('Done, the result is saved at \n{0}\n{1}'.format(out_train_fn, out_val_fn))


def main(args):
    win_size = args.win_size
    min_r = args.ratio
    min_f = args.frequency
    train_rate = args.train_rate

    in_data_root_dir = args.in_data_root_dir
    out_data_root_dir = args.out_data_root_dir

    nb_cls_prop = args.nb_cls_prop
    n_cpu = args.n_cpu

    # input data dir
    in_win_data_dir = os.path.join(in_data_root_dir, 'win_{}'.format(win_size))
    # input data fname, the dataset will be split into train data set and validation data set.
    train_orignal_fn = os.path.join(in_win_data_dir,
                                    'trains_mr{0:.2f}_mf{1:.2f}_original.h5'.format(min_r, min_f))
    if not os.path.exists(train_orignal_fn):
        raise FileNotFoundError('all training data to be split not found: {}'.format(train_orignal_fn))

    out_win_data_dir = os.path.join(out_data_root_dir, 'win_{}'.format(win_size))
    if not os.path.isdir(out_win_data_dir):
        os.mkdir(out_win_data_dir)

    out_train_fn = os.path.join(out_win_data_dir,
                                'trains_mr{0:.2f}_mf{1:.2f}_train_{2}_tr{3:.2f}.h5'.format(
                                    min_r, min_f, ''.join(nb_cls_prop.split(':')), train_rate))

    out_val_fn = os.path.join(out_win_data_dir,
                              'trains_mr{0:.2f}_mf{1:.2f}_val_{2}_tr{3:.2f}.h5'.format(
                                  min_r, min_f, ''.join(nb_cls_prop.split(':')), train_rate))
    if os.path.exists(out_train_fn):
        os.remove(out_train_fn)
    if os.path.exists(out_val_fn):
        os.remove(out_val_fn)

    create_train_val_dataset(train_orignal_fn,
                             out_train_fn, out_val_fn, win_size,
                             train_ratio=train_rate, nb_cls_prop=nb_cls_prop, n_proc=n_cpu)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Create training and validation data set ')

    parser.add_argument(
        "-w",
        "--win_size",
        type=int,
        default=1000,
        help='window size. This will be hyperparamter')

    parser.add_argument(
        "-r",
        "--ratio",
        type=float,
        default=0.1,
        help='cnv region that has read coverage less than the ratio will be filtered out. This will be hyperparameter')

    parser.add_argument(
        "-q",
        "--frequency",
        type=float,
        default=0.01,
        help='cnv whose frequency less than the frequency will be filtered out. This will be hyperparameter')

    parser.add_argument(
        "-t",
        "--train_rate",
        type=float,
        default=0.8,
        help="split rate for train samples")

    parser.add_argument(
        "-b",
        "--nb_cls_prop",
        type=str,
        default='1:1:1',
        help="the number of samples between classes. DEL:DUP:NEU")

    parser.add_argument(
        "-p",
        "--n_cpu",
        type=int,
        default=20,
        help="the number of cpu")

    parser.add_argument(
        "-i",
        "--in_data_root_dir",
        type=str,
        default='/zfssz2/ST_MCHRI/BIGDATA/PROJECT/NIPT_CNV/f_cnv_out/data',
        help="input dir for train all data set file: w{0}_r{1:.2f}_f{2:.2f}_train_all.csv")

    parser.add_argument(
        "-o",
        "--out_data_root_dir",
        type=str,
        default='/zfssz2/ST_MCHRI/BIGDATA/PROJECT/NIPT_CNV/f_cnv_out/data',
        help="out dir for train and validation")

    args = parser.parse_args()
    logger.info('args: {}'.format(args))
    main(args)
